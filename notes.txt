Den moy aresoyn ta positive Negative, epireasmenos apo to sequence analysis in RNN
einai san na vazoyme kati xierotero (to nltk) na apofasizei an kati einai thetiko h arnhtiko kai oxi to NN poy einai h doyleia toy ayth
paradeigma 'negative not'

I found how to get the dictionary of Keras tokenizer, it is in guys code about CNN  

The best training accuracy is the second method (learn word embedding from scratch) at 90.52%. 
!!!!!!!!!!!!!!!!!!!!



conv-relu-conv-relu-pool

Convolutional networks may include local or global pooling layers,[clarification needed] which combine the outputs of neuron clusters at one layer into a single neuron in the next layer.[10][11] For example, max pooling uses the maximum value from each of a cluster of neurons at the prior layer.[12] Another example is average pooling, which uses the average value from each of a cluster of neurons at the prior layer.[13]
??which and why??



Now we are done with the data preparation. Before we jump into CNN, I would like to test one more thing (sorry for the delay). When we feed this sequential vector representation of data, we will use Embedding layer in Keras. With Embedding layer, I can either pass pre-defined embedding, which I prepared as ‘embedding_matrix’ above, or Embedding layer itself can learn word embeddings as the whole model trains. And another possibility is we can still feed the pre-defined embedding but make it trainable so that it will update the values of vectors as the model trains.

find the max-lenght of a tweet

-Use dropout? yes, no and why
- which activation functions and why
-same for learning rate


-Use pandas cols = 'text', 'label' so as when split to get the correct label
- Should we add the label immediately after processing the line? Not-necessary (is dobe correctly)



For glove!!!!!!!
for i, line in enumerate(open('data/wiki-news-300d-1M.vec')):
    values = line.split()
    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')


# convert text to sequence of tokens and pad them to ensure equal length vectors 
train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)
valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)


For example, it may no-longer make sense to stem words or remove punctuation for contractions.

At the output layer, we have only one neuron as we are solving a binary classification problem (predict 0 or 1). We could also have two neurons for predicting each of both classes.
	wh as weight matrix to the hidden layer
	bh as bias matrix to the hidden layer
	wout as weight matrix to the output layer
	bout as bias matrix to the output layer



---------
- Emphasize emoji ---> show sentiment!!!!!
- words in asterisks
- : l
- There are repeated tweets - //TODO: check this

- Experiment history. If we do not like a result the solution is to improve the model - not to avoid include it in the report
- FastText?
- Validation: Particle Swarm Model Selection (PSO) ??




SMER preproc they do:
-elongated
-contraction:
    tweets = tweets.str.replace('n\'t', ' not', case=False)
    tweets = tweets.str.replace('i\'m', 'i am', case=False)
    tweets = tweets.str.replace('\'re', ' are', case=False)
    tweets = tweets.str.replace('it\'s', 'it is', case=False)
    tweets = tweets.str.replace('that\'s', 'that is', case=False)
    tweets = tweets.str.replace('\'ll', ' will', case=False)
    tweets = tweets.str.replace('\'l', ' will', case=False)
    tweets = tweets.str.replace('\'ve', ' have', case=False)
    tweets = tweets.str.replace('\'d', ' would', case=False)
    tweets = tweets.str.replace('he\'s', 'he is', case=False)
    tweets = tweets.str.replace('what\'s', 'what is', case=False)
    tweets = tweets.str.replace('who\'s', 'who is', case=False)
    tweets = tweets.str.replace('\'s', '', case=False)
-emoji  
-hashtag
-numbers 
-sentiment



A multilayer perceptron (MLP) is a class of feedforward artificial neural network. An MLP consists of, at least, three layers of nodes: an input layer, a hidden layer and an output layer. Except for the input nodes, each node is a neuron that uses a nonlinear activation function. MLP utilizes a supervised learning technique called backpropagation for training.[1][2] Its multiple layers and non-linear activation distinguish MLP from a linear perceptron. It can distinguish data that is not linearly separable.[3]


#loading the glove model
def loadGloveModel(gloveFile):
    print("Loading Glove Model")
    f = open(gloveFile,'r')
    model = {}
    for line in f:
        splitLine = line.split()
        word = splitLine[0]
        embedding = [float(val) for val in splitLine[1:]]
        model[word] = embedding
    print ("Done."),len(model),(" words loaded!")
    return model

# save the glove model
model=loadGloveModel("/mnt/hdd/datasets/glove/glove.twitter.27B.200d.txt")

!!!!!!!!!!! Kai glitwnw teleiws apo to gamw glove 