The best training accuracy is the second method (learn word embedding from scratch) at 90.52%. 
!!!!!!!!!!!!!!!!!!!!



conv-relu-conv-relu-pool

Convolutional networks may include local or global pooling layers,[clarification needed] which combine the outputs of neuron clusters at one layer into a single neuron in the next layer.[10][11] For example, max pooling uses the maximum value from each of a cluster of neurons at the prior layer.[12] Another example is average pooling, which uses the average value from each of a cluster of neurons at the prior layer.[13]
??which and why??



Now we are done with the data preparation. Before we jump into CNN, I would like to test one more thing (sorry for the delay). When we feed this sequential vector representation of data, we will use Embedding layer in Keras. With Embedding layer, I can either pass pre-defined embedding, which I prepared as ‘embedding_matrix’ above, or Embedding layer itself can learn word embeddings as the whole model trains. And another possibility is we can still feed the pre-defined embedding but make it trainable so that it will update the values of vectors as the model trains.

find the max-lenght of a tweet

-Use dropout? yes, no and why
- which activation functions and why
-same for learning rate


-Use pandas cols = 'text', 'label' so as when split to get the correct label
- Should we add the label immediately after processing the line? Not-necessary (is dobe correctly)



For glove!!!!!!!
for i, line in enumerate(open('data/wiki-news-300d-1M.vec')):
    values = line.split()
    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')


# convert text to sequence of tokens and pad them to ensure equal length vectors 
train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)
valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)


For example, it may no-longer make sense to stem words or remove punctuation for contractions.

At the output layer, we have only one neuron as we are solving a binary classification problem (predict 0 or 1). We could also have two neurons for predicting each of both classes.
	wh as weight matrix to the hidden layer
	bh as bias matrix to the hidden layer
	wout as weight matrix to the output layer
	bout as bias matrix to the output layer



---------
- Emphasize emoji ---> show sentiment!!!!!
- words in asterisks
- : l
- There are repeated tweets - //TODO: check this

- Experiment history. If we do not like a result the solution is to improve the model - not to avoid include it in the report
- FastText?
- Validation: Particle Swarm Model Selection (PSO) ??




SMER preproc they do:
-elongated
-contraction:
    tweets = tweets.str.replace('n\'t', ' not', case=False)
    tweets = tweets.str.replace('i\'m', 'i am', case=False)
    tweets = tweets.str.replace('\'re', ' are', case=False)
    tweets = tweets.str.replace('it\'s', 'it is', case=False)
    tweets = tweets.str.replace('that\'s', 'that is', case=False)
    tweets = tweets.str.replace('\'ll', ' will', case=False)
    tweets = tweets.str.replace('\'l', ' will', case=False)
    tweets = tweets.str.replace('\'ve', ' have', case=False)
    tweets = tweets.str.replace('\'d', ' would', case=False)
    tweets = tweets.str.replace('he\'s', 'he is', case=False)
    tweets = tweets.str.replace('what\'s', 'what is', case=False)
    tweets = tweets.str.replace('who\'s', 'who is', case=False)
    tweets = tweets.str.replace('\'s', '', case=False)
-emoji  
-hashtag
-numbers 
-sentiment