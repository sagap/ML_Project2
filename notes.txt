-setup glove and word2vec
-Check with tfidf which is the best preprocessing
-show learnign curve for our algorithms
	train, test error




untitled: foivos	
Test-Preprocessing.ipynb : foivos
BoW-Copy1: foivos
BoW: stelios keep
BoW_preproc: stelios keeep


preproc they do:

-elongated
-contraction:
    tweets = tweets.str.replace('n\'t', ' not', case=False)
    tweets = tweets.str.replace('i\'m', 'i am', case=False)
    tweets = tweets.str.replace('\'re', ' are', case=False)
    tweets = tweets.str.replace('it\'s', 'it is', case=False)
    tweets = tweets.str.replace('that\'s', 'that is', case=False)
    tweets = tweets.str.replace('\'ll', ' will', case=False)
    tweets = tweets.str.replace('\'l', ' will', case=False)
    tweets = tweets.str.replace('\'ve', ' have', case=False)
    tweets = tweets.str.replace('\'d', ' would', case=False)
    tweets = tweets.str.replace('he\'s', 'he is', case=False)
    tweets = tweets.str.replace('what\'s', 'what is', case=False)
    tweets = tweets.str.replace('who\'s', 'who is', case=False)
    tweets = tweets.str.replace('\'s', '', case=False)
-emoji  
-hashtag
-numbers 
-sentiment

# def do_preprocessing(filepath, test_file=False):
    
#     initial_file = filepath.split('/')[-1]
#     new_file = initial_file[:-4] + '_processed'
#     print('preprocessing', initial_file)
    
#     with open(filepath, 'r') as f_in:
#         lines = f_in.readlines()
#         f_in.close()    
#     # TODO remove it
#     lines = lines [:2]
    
#     if test_file:
#         lines = [line.split(',', 1)[1] for line in lines]
    
#     processed_list = []
#     for line in tqdm(lines):
#         pro_line = line
#         pro_line = remove_tags(pro_line)
#         pro_line = replace_slang(pro_line)
#         pro_line = replace_contraction(pro_line)
#         pro_line = replace_emoji(pro_line)
#         pro_line = convert_to_lowercase(pro_line)
#         pro_line = filter_digits(pro_line)
#         pro_line = replace_elongated(pro_line)
#         pro_line = separate_hashtags(pro_line)
#         pro_line = remove_new_line(pro_line)
#         processed_list.append(pro_line)

#     helpers.write_file(processed_list, new_file, test_file)

Emphasize emoji ---> show sentiment!!!!!

words in asterisks

-----0. General Info-----

import nltk, nltk.download('wordnet')
- haaappyyyy
- : l
- There are repeated tweets - //TODO: check this
- All tweets have been tokenized already, so that the words and punctuation are properly separated by a whitespace.
- the labels indicate if a tweet used to contain a :) or :( smiley. (1 or -1)
- Allowed to use scikit-learn and external libraries
- Each participant 5 submissions per day(10 per team). Wrong submission file format do not count.
- Metric: Misclassification error score. Accuracy



-----1. Preprocessing-----

- No Preprocessing in the 1st run


-----2. Text representation-----

- Naive-first approach:
  Load the training tweets and the built GloVe word embeddings. Using the word embeddings, construct a feature representation of each training tweet (by averaging the word vectors over all words of the tweet).




-----3. Train a Linear Classifier----- 

- 1. Logistic regression - scikit-learn
- 2. SVM  - scikit-learn



-----4. Prediction - Submission/Evaluation-----

- Memory errors when run full dataset locally => need more powerful computer
- Very important to build again accurate local testing
- Experiment history. If we do not like a result the solution is to improve the model - not to avoid include it in the report



----- Notes about the project---

lemmatization and stemming)

1. Preprocessing
- contractions expansion: don't -> do (check if have been separated in the tokenization phase)
- Emojis Transformation: check if we have
- check if we have hashtags - if yes: split word
- POS Tagging is it necessary? -> yes for lemmatization


2. Text Representation
- word embeddings
- tweet embeddings
- bag of words
- pagraph embeddings

3. ML algorithms            IMPORTANT give your ml method as argument in the method and let it compute the accuracy
- Naive bayes. should we??
- FastText?
- Neural Network (Convolutional) 
- Validation: Particle Swarm Model Selection (PSO) ??
