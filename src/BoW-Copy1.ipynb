{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from functools import reduce\n",
    "import preprocessing as preproc\n",
    "import time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import helpers\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "import operator\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "# from math import log\n",
    "# from options import WORD_FREQUENCIES"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Prepocessning:\n",
    "3) HASHTAGS\n",
    "6) slangs\n",
    "7) keep only words with num of reps >=3\n",
    "8) discard single char words"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from collections import Counter\n",
    "joined_lines = ' '.join(sub_list)\n",
    "c = Counter(joined_lines.split(' '))\n",
    "print(len(c))\n",
    "print(c)\n",
    "non_frequent = [word for word,freq in c.items() if freq < 3 ]\n",
    "print(non_frequent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../twitter-datasets/train_pos.txt') as pos_in, open(\n",
    "    '../twitter-datasets/train_neg.txt') as neg_in:\n",
    "    pos_lines = pos_in.readlines()\n",
    "    neg_lines = neg_in.readlines()\n",
    "    pos_in.close()\n",
    "    neg_in.close()\n",
    "lines = pos_lines + neg_lines\n",
    "lines = [line.replace('\\n', '') for line in lines]\n",
    "print(len(lines))\n",
    "print(lines[0])\n",
    "lines_copy = lines.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_occurence(text_list):\n",
    "    \n",
    "    def find_punct(text):\n",
    "        return len(re.findall('! ', text))\n",
    "    \n",
    "    counter = 0\n",
    "    for line in text_list:\n",
    "        counter += find_punct(line)\n",
    "    print(counter)\n",
    "    \n",
    "count_occurence(lines_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_emoji(text):\n",
    "    rep_text = text\n",
    "    rep_text = re.sub(':(-)?@', ' angry_emoji ', rep_text)\n",
    "    rep_text = re.sub(':( )?\\$', ' blushing_emoji ', rep_text)\n",
    "    rep_text = re.sub('>:\\)|>:D|>:-D|>;\\)|>:-\\)|}:-\\)|}:\\)|3:-\\)|3:\\)', ' devil_emoji ', rep_text) #done\n",
    "    rep_text = re.sub('O:-\\)|0:-3|0:3|0:-\\)|0:\\)|0;^\\)', ' angel_emoji ', rep_text)\n",
    "    rep_text = re.sub(':( )?\\)+|\\(+:|:-\\)+|\\(+-:|:\\}| c : |:( )?O( )?\\)|:( )?-( )?]|^( )?-( )?^', ' happy_emoji ', rep_text, flags=re.I)\n",
    "    rep_text = re.sub(\n",
    "        ':-D|:D|=D|=-D|8-D|8D|x-D|xD|X-D|=-D|:( )?d|:-d|>:d|=3|=-3|:\\'-\\)|:\\'\\)|\\(\\':|\\[:|:\\]', ' happy_emoji ', rep_text)    \n",
    "    rep_text = re.sub('\\)+:|:\\(+|:-\\(+|\\)+-:|>:\\[| : c |:\\||:-\\[', ' sad_emoji ', rep_text)\n",
    "    rep_text = re.sub(':( )?\\*|:( )?-( )?\\*+|:x', ' kiss_emoji ', rep_text, flags=re.I) \n",
    "    rep_text = re.sub('<3', ' heart ', rep_text)\n",
    "    rep_text = re.sub(';-\\)|;\\)|\\*\\)|\\*-\\)|;-\\]|;]|;D|;\\^\\)', ' wink_emoji ', rep_text)\n",
    "    rep_text = re.sub('>:P|:-P|:P|X-P|xp|=p|:b|:-b|;p| : p ', ' tongue_emoji ', rep_text, flags=re.I)\n",
    "    rep_text = re.sub('>:O|:( )?-( )?O|:( )?O', ' surprise_emoji ', rep_text, flags=re.I) #done\n",
    "    rep_text = re.sub(\n",
    "    ':-\\||<:-\\||>( )?.( )?<|:( )?-( )?\\/|:( )?-( )?\\\\\\\\|:S|:( )?\\/|=\\/|=( )?\\\\\\\\|:( )?\\\\\\\\|:( )?-( )?s|;( )?/|:( )?l ',\n",
    "    ' skeptical_emoji ', rep_text) #done\n",
    "    return rep_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' happy_emoji '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc.replace_emoji(':)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'../twitter-datasets/words-by-frequency.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def emphasize_sentiment_words(tweet):\n",
    "    \"\"\"\n",
    "    DESCRIPTION: \n",
    "                By using an opinion lexicon, if a tweet contained a positive or negative word\n",
    "                 from that lexicon, it is emphasized respectively\n",
    "    INPUT: \n",
    "            tweet: a tweet as a python string\n",
    "    OUTPUT: \n",
    "            a transformed tweet as a python string\n",
    "    \"\"\"\n",
    "    t = []\n",
    "    for w in tweet.split():\n",
    "        if w in positiveWords:\n",
    "            t.append('positive ' + w)\n",
    "        elif w in negativeWords:\n",
    "            t.append('negative ' + w)\n",
    "        else:\n",
    "            t.append(w)\n",
    "    return (\" \".join(t)).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer() # stop_words = stopwords.words('english')\n",
    "X = count_vect.fit_transform(lines)\n",
    "vocab = count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.zeros(shape=(len(lines)))\n",
    "y[:len(pos_lines)] = 1\n",
    "y[len(pos_lines):] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression().fit(X, y)\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=kf)\n",
    "print(\"Accuracy: %0.4f (+/- %0.4f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Accuracy: 0.8093 (+/- 0.0028) : no preproc\n",
    "Accuracy: 0.7966 (+/- 0.0033 : stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "# elapsed_time = divmod(round((time.time() - start_time)), 60)\n",
    "# print('------\\nElapsed time: {m} min {s} sec\\n'.format(m=elapsed_time[0], s=elapsed_time[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
