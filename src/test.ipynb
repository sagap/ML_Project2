{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import text_representation as repre\n",
    "import helpers\n",
    "import preprocessing as preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../twitter-datasets/train_pos.txt', 'r') as pos_in, open('../twitter-datasets/train_neg.txt', 'r') as neg_in:\n",
    "    pos_lines = pos_in.readlines()\n",
    "    neg_lines = neg_in.readlines()\n",
    "    pos_in.close()\n",
    "    neg_in.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos_processed = []\n",
    "for line in pos_lines:\n",
    "    pro_line = line\n",
    "    pro_line = preproc.remove_unnecessary(pro_line)\n",
    "    pro_line = preproc.replace_contraction(pro_line)\n",
    "    pro_line = preproc.replace_numbers(pro_line)\n",
    "    pro_line = preproc.replace_emoji(pro_line)\n",
    "#     pro_line = preproc.replace_elongated_word(pro_line)\n",
    "    pos_processed.append(pro_line)\n",
    "    \n",
    "helpers.write_file(pos_processed, 'train_pos_processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_processed = []\n",
    "for line in neg_lines:\n",
    "    pro_line = line\n",
    "    pro_line = preproc.remove_unnecessary(pro_line)\n",
    "    pro_line = preproc.replace_contraction(pro_line)\n",
    "    pro_line = preproc.replace_numbers(pro_line)\n",
    "    pro_line = preproc.replace_emoji(pro_line)\n",
    "#     pro_line = preproc.replace_elongated_word(pro_line)\n",
    "    neg_processed.append(pro_line)\n",
    "\n",
    "helpers.write_file(neg_processed, 'train_neg_processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "print(len(pos_processed))\n",
    "print(len(neg_processed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = repre.load_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = repre.create_dict_from_provided_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_path = '../twitter-datasets/train_pos_processed.txt'\n",
    "pos_features = repre.create_tweet_features(train_pos_path, vocab_dict, shape_of_word_embeddings=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg_path = '../twitter-datasets/train_neg_processed.txt'\n",
    "neg_features = repre.create_tweet_features(train_neg_path, vocab_dict, shape_of_word_embeddings=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_features = pos_features[1:]\n",
    "neg_features = neg_features[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_shape = pos_features.shape[0]\n",
    "neg_shape = neg_features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack((pos_features, neg_features))\n",
    "y = np.zeros(shape=(pos_features.shape[0] + neg_features.shape[0]))\n",
    "y[:pos_shape] = 1\n",
    "y[pos_shape:] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.,  1.])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99990"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rs = ShuffleSplit(n_splits=4, test_size=.25, random_state=0)\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5966 (+/- 0.0039)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf, X, y, cv=kf)\n",
    "print(\"Accuracy: %0.4f (+/- %0.4f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LOAD TEST SET AND PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = '../twitter-datasets/test_data.txt'\n",
    "\n",
    "with open(test_path, 'r') as test_in:\n",
    "    test_lines = test_in.readlines()\n",
    "    test_in.close()\n",
    "\n",
    "test_lines = [','.join(line.split(',')[1:]) for line in test_lines]\n",
    "test_processed = []\n",
    "for line in test_lines:\n",
    "    pro_line = line\n",
    "    pro_line = preproc.remove_unnecessary(pro_line)\n",
    "    pro_line = preproc.replace_contraction(pro_line)\n",
    "    pro_line = preproc.replace_numbers(pro_line)\n",
    "    pro_line = preproc.replace_emoji(pro_line)\n",
    "#     pro_line = preproc.replace_elongated_word(pro_line)\n",
    "    test_processed.append(pro_line)\n",
    "    \n",
    "helpers.write_file(test_processed, 'test_data_processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tweet_features(path_to_file, vocab_dict, shape_of_word_embeddings):\n",
    "    '''give path to file and vocabulary dictionary\n",
    "        and return features of the tweets'''\n",
    "    features = np.empty(shape=shape_of_word_embeddings)\n",
    "    with open(path_to_file, 'r') as file_in:\n",
    "        lines = file_in.readlines()    \n",
    "        for line in lines:\n",
    "            tweet = line.split(' ')\n",
    "            tweet_features = np.zeros(shape=(len(tweet), shape_of_word_embeddings))\n",
    "            for i in range(0, len(tweet)):\n",
    "                if tweet[i] in vocab_dict:\n",
    "                    tweet_features[i] = vocab_dict[tweet[i]]\n",
    "            tweet_features = np.mean(tweet_features, axis=0)\n",
    "            features = np.vstack((features, tweet_features))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_processed_path = '../twitter-datasets/test_data_processed.txt'\n",
    "test_features = create_tweet_features(test_processed_path, vocab_dict, shape_of_word_embeddings=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 20)\n"
     ]
    }
   ],
   "source": [
    "test_features = test_features[1:]\n",
    "print(test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "helpers.create_submission_csv(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.,  1.])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
